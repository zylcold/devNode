# 1. 卡顿优化
## 了解CPU和GPU
在屏幕成像过程中，CPU和GPU的作用是至关重要的。

* **CPU**
- Central Processing Unit，中央处理器，在iOS程序中，负责对象的创建和销毁、对象属性的调整、布局的计算、文本的计算和排版规格、图片的格式转码和解码、图像的绘制（**Core Graphic**）
* **GPU** 
- Graphics Processing Unit，图形处理器，负责纹理的渲染。如果没有接触过OpenGL的朋友，可能不太好理解纹理渲染这个概念，我们知道，屏幕上面的物理元件是像素，我们在屏幕上面看到的图片，文字，视频，就是由屏幕上的所有像素，通过控制色值变化而呈现出来的。那么像素的色值数据，就是由GPU计算得出的，然后将这些数据提交给视频控制器，由它负责显示到屏幕上。

![[dfbcc46f8d564984b762349e40995506~tplv-k3u1fbpfcp-zoom-1.image.png]]

iOS中采用的是双缓冲机制，分为前帧缓存和后帧缓存。

## 屏幕成像原理
屏幕成像原理是一个非常庞大的知识体系，这里仅介绍一下我们当前所需要了解的部分，以便我们接下来的话题。 屏幕的显示，是受控于两种信号
* **垂直同步信号（VSync）**
屏幕发出VSync之后，就表示将要进行新一帧画面的显示，于是开始从帧缓存里面读取经过GPU渲染好的用于显示的数据

* **水平同步信号（HSync）**
显示器从帧缓存里拿到数据之后，是从上到下一行一行的刷新的，刷新完一行，就发出一个HSync，直到最下面一层显示出来，这样，一帧的画面就完成了显示。

![[7b29e97c3c344296b64dc654ae14244b~tplv-k3u1fbpfcp-zoom-1.image.png]]

我们可以把屏幕想象成刷墙师傅，每一帧的数据就是桶里的油漆，而GPU就是负责提供油漆的店老板。

## 卡顿产生的原因
我们手机屏幕的刷帧率是60FPS（**Frame per Second** 帧/秒），也就是会所1秒钟的时间，屏幕可以刷新60帧（次）。完成一帧刷新的用时是16.6毫秒。因此垂直同步信号**VSync**就是每16.6毫秒发出一次。

两次**VSync**之间的这16.6毫秒，就是被CPU和GPU共同完成下一帧画面的计算和渲染工作的时间。但是CPU计算和GPU渲染所用的时间是取决于任务的运算量的，因此就有可能大于16.6毫秒，也有可能小于或者等于16.6毫秒。

这里我们假设Tc=CPU计算时间，Tg=GPU渲染时间。如果Tc+Tg <= 16.6ms，那么完美，下一帧画面的数据可以在**VSync**到来之间就准备好；但是如果Tc+Tg > 16.6ms，意味着屏幕将要开始显示下一帧画面了，但是CPU和GPU那里却还在咔咔咔的准备着画面数据，那么没办法，在接下来的**16.6ms**周期里面，屏幕就继续用上一帧的画面数据来显示。同一个画面被显示了过长的时间，就造成了视觉上可感知到的卡顿现象。再通过下图来体会一下
![[8fffc98e674c4da59bc9e67f7cd30473~tplv-k3u1fbpfcp-zoom-1.image.png]]

上看我们了解了产生的原因，就是由于CPU计算时间和GPU的渲染时间过长导致的。因此想要优化卡顿问题，无非就是从CPU和GPU下手，减轻它们的工作量，以控制它们的操作耗时。

## 卡顿优化-CPU
首先开看看CPU，我们有如下途径来减轻它的计算任务

尽量用轻量级的对象，比如简单的数字，尽量选择基础数据类型，不要使用NSNumber，对象操作的开销肯定大于基础数据类型的开销。

CALayer是用来显示图像的，UIView是负责处理触摸交互事件的，UIView内部封装了CALayer属性，因此UIView的图像显示实际上是它内部的这个CALayer来完成的。因此如果我们不需要考虑触摸事件，只是单纯的要显示内容的话，可以考虑用CALayer取代UIView。

尽量提前计算好布局，在有需要的时候一次性调整对应属性，不要多次修改图片的size最好是跟UIImageView的size刚好一致，可以省去图片剪裁的操作开销控制一下线程的最大并发数

尽量把耗时操作放到子线程处理（比如文本的尺寸计算、绘制，图片的解码、绘制）

## 卡顿优化-GPU
对与GPU，有下列方案可以减少渲染开销
* 尽量避免段时间内大量的图片显示，尽可能将多张图片合成一张图片显示，比如说三张图片同时显示，不如将这三张图片合成到一块作为一张图片来显示

* GPU能处理的最大纹理尺寸是4096*4096，一旦超过这个尺寸，就会占用CPU资源进行处理，这样势必影响CPU的运算效率，因此纹理尽量不要超过这个尺寸

* 尽量减少视图的数量和层级

* 减少不必要的透明的视图

* 尽量避免离屏渲染

### 离屏渲染
在OpenGL中，GPU有两种渲染方式
* **On-Screen Rendering**：当前屏幕渲染，在当前用于显示的屏幕缓冲区进行渲染操作
* **Off-Screen Rendering**：离屏渲染，在当前屏幕缓冲区以外开辟一个新的缓冲区进行渲染操作

为什么离屏渲染消耗性能？
* 需要创建新的缓冲区
* 离屏渲染的整个过程中，需要多次切换上下文环境，先是从当前屏幕缓冲区（**On-Screen**）切换到离屏缓冲区(**Off-Screen**)，等完成离屏渲染操作之后，将离屏缓冲区的渲染结果显示到屏幕上，然后还需要将上下文环境从离屏缓冲区切换回当前屏幕缓冲区。

哪些操作会触发离屏渲染？
* 光栅化操作 layer.shouldRasterize = YES
* 遮罩设置 layer.mask
* 圆角设置 layer.masksToBounds = YES&layer.cornerRadius>0
🥝可以考虑通过CoreGraphics绘制剪裁圆角，或者叫美工提供圆角图片🥝
* 阴影设置 layer.shadowXXX
🥝如果设置了layer.shadowPath就不会产生离屏渲染🥝

## 卡顿检测
平时我们所碰到的卡顿，主要是在主线程执行了比较耗时的操作，可以在主线程**RunLoop**中添加observer，通过监听**RunLoop**的状态切换耗时，来监控卡顿。